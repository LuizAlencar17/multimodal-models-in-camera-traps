{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from uuid import uuid4\n",
    "\n",
    "FILE_TYPE = \".JPG\"\n",
    "IMAGES_PATH = \"/data/luiz/dataset/serengeti_images/\"\n",
    "ANNOTATIONS_PATH = \"/data/luiz/dataset/serengeti/SnapshotSerengeti_S1-11_v2.1.json\"\n",
    "SEQUENCE_PATH = \"/ssd/luiz/dataset/sequences/\"\n",
    "RESULTS_PATH = \"/data/luiz/dataset/partitions/\"\n",
    "ANNOTATIONS_PATH_CSV = \"/data/luiz/dataset/serengeti/SnapshotSerengeti_v2_1_annotations.csv\"\n",
    "DATABASE = \"serengeti\"\n",
    "CSV_FIELDS = {\n",
    "    'capture_id': 'id',\n",
    "    'question__standing': 'standing', \n",
    "    'question__resting': 'resting', \n",
    "    'question__moving': 'moving', \n",
    "    'question__eating': 'eating', \n",
    "    'question__interacting': 'interacting'\n",
    "}\n",
    "USE_CSV = True\n",
    "\n",
    "SEED = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24469/234524135.py:4: DtypeWarning: Columns (8,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_csv = pd.read_csv(ANNOTATIONS_PATH_CSV)[CSV_FIELDS.keys()]\n"
     ]
    }
   ],
   "source": [
    "data = json.load(open(ANNOTATIONS_PATH))\n",
    "\n",
    "if USE_CSV:\n",
    "    df_csv = pd.read_csv(ANNOTATIONS_PATH_CSV)[CSV_FIELDS.keys()]\n",
    "    df_data = pd.DataFrame(data[\"annotations\"])\n",
    "    df_data = pd.merge(df_data, df_csv, left_on='seq_id', right_on='capture_id', how='inner')\n",
    "\n",
    "    df_data = df_data.rename(columns=CSV_FIELDS)\n",
    "    # Remover duplicatas mantendo a primeira ocorrência\n",
    "    df_data = df_data.loc[:, ~df_data.columns.duplicated()]\n",
    "\n",
    "data[\"annotations\"] = df_data.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting sequence mapper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7429835/7429835 [01:43<00:00, 71498.02it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting categories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1033219/1033219 [00:11<00:00, 93374.92it/s]\n"
     ]
    }
   ],
   "source": [
    "ALL_ACTIONS = ['standing', 'resting', 'moving', 'eating', 'interacting']\n",
    "ACTIONS = ['resting', 'moving', 'eating']\n",
    "CATEGORIES_INVALID = [1]\n",
    "MAX_ANIMALS = 1\n",
    "\n",
    "def get_category_from_sequence(sequence):\n",
    "    mapper = {}\n",
    "    for action in ALL_ACTIONS:\n",
    "        for frame in sequence:\n",
    "            mapper[action] = mapper.get(action, 0) + frame.get(action, 0)\n",
    "    return max(mapper, key=mapper.get)\n",
    "\n",
    "def is_valid_frame(frame):\n",
    "    try:\n",
    "        count = int(frame[\"count\"])\n",
    "    except Exception:\n",
    "        count = 0\n",
    "    exist_file = os.path.isfile(frame[\"path\"])\n",
    "    category_invalid = frame[\"category_id\"] in CATEGORIES_INVALID\n",
    "    count_valid = count <= MAX_ANIMALS\n",
    "    if not exist_file or category_invalid or not count_valid:\n",
    "        return False\n",
    "    # for action in ALL_ACTIONS:\n",
    "    #     try:\n",
    "    #         if not frame.get(action) >= 0:\n",
    "    #             return False\n",
    "    #     except Exception:\n",
    "    #         return False\n",
    "    return True\n",
    "\n",
    "def get_sequence_mapper(annotations):\n",
    "    mapper = {}\n",
    "    print(\"getting sequence mapper\")\n",
    "    for item in tqdm(annotations):\n",
    "        id = item.get(\"seq_id\")\n",
    "        item[\"path\"] = f'{IMAGES_PATH}{item[\"image_id\"]}{FILE_TYPE}'\n",
    "        if is_valid_frame(item):\n",
    "            if not mapper.get(id):\n",
    "                mapper[id] = []\n",
    "            mapper[id].append(item)\n",
    "    return mapper\n",
    "\n",
    "def get_frames_sequences(data):\n",
    "    sequence_mapper = get_sequence_mapper(data[\"annotations\"])\n",
    "    events = []\n",
    "    print(\"getting categories\")\n",
    "    for key in tqdm(sequence_mapper.keys()):\n",
    "        frames = sequence_mapper[key]\n",
    "        events.append({\n",
    "            \"num_frames\": len(frames),\n",
    "            \"frames\": frames,\n",
    "            \"datetime\": frames[0][\"datetime\"],\n",
    "            \"category\": get_category_from_sequence(frames)})\n",
    "    return events\n",
    "\n",
    "sequences = get_frames_sequences(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['resting', 'moving', 'eating'], dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(sequences)\n",
    "df = df[df.num_frames > 1]\n",
    "df = df.replace(\"interacting\", \"moving\").replace(\"standing\", \"resting\")\n",
    "df.category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(df):\n",
    "    dfs = []\n",
    "    size = min(df.category.value_counts())\n",
    "    for category in df.category.unique():\n",
    "        filtered = df[df.category == category].sample(size, random_state=SEED)\n",
    "        dfs.append(filtered)\n",
    "    new_df = pd.concat(dfs).reset_index(drop=True)\n",
    "    return new_df.sample(len(new_df), random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "def get_empty_in_frames(frames):\n",
    "    for frame in frames:\n",
    "        if frame[\"category_id\"] == 0:\n",
    "            return frame[\"path\"]\n",
    "    return None\n",
    "\n",
    "def get_animal_in_frames(frames):\n",
    "    for frame in frames:\n",
    "        if frame[\"category_id\"] != 0:\n",
    "            return frame[\"path\"]\n",
    "    return None\n",
    "\n",
    "df[\"path_empty\"] = df.frames.map(lambda a: get_empty_in_frames(a))\n",
    "df[\"path_animal\"] = df.frames.map(lambda a: get_animal_in_frames(a))\n",
    "df['location'] = df.frames.map(lambda a: a[0]['location'])\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df['path_seq'] = df.frames.map(lambda a: \",\".join([item[\"path\"] for item in a]))\n",
    "\n",
    "df = df[[\"num_frames\", \"frames\", \"category\", \"location\", \"datetime\", \"path_empty\", \"path_animal\", \"path_seq\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "resting    792905\n",
       "moving      56684\n",
       "eating      28968\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = balance_dataset(df)\n",
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_frames</th>\n",
       "      <th>frames</th>\n",
       "      <th>category</th>\n",
       "      <th>location</th>\n",
       "      <th>datetime</th>\n",
       "      <th>path_empty</th>\n",
       "      <th>path_animal</th>\n",
       "      <th>path_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>[{'sequence_level_annotation': True, 'id': '10...</td>\n",
       "      <td>resting</td>\n",
       "      <td>B04</td>\n",
       "      <td>2010-08-12 16:28:50</td>\n",
       "      <td>/data/luiz/dataset/serengeti_images/S1/B04/B04...</td>\n",
       "      <td>None</td>\n",
       "      <td>/data/luiz/dataset/serengeti_images/S1/B04/B04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>[{'sequence_level_annotation': True, 'id': '10...</td>\n",
       "      <td>moving</td>\n",
       "      <td>B04</td>\n",
       "      <td>2010-08-20 03:37:00</td>\n",
       "      <td>None</td>\n",
       "      <td>/data/luiz/dataset/serengeti_images/S1/B04/B04...</td>\n",
       "      <td>/data/luiz/dataset/serengeti_images/S1/B04/B04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>[{'sequence_level_annotation': True, 'id': '10...</td>\n",
       "      <td>eating</td>\n",
       "      <td>B05</td>\n",
       "      <td>2010-07-20 15:19:52</td>\n",
       "      <td>None</td>\n",
       "      <td>/data/luiz/dataset/serengeti_images/S1/B05/B05...</td>\n",
       "      <td>/data/luiz/dataset/serengeti_images/S1/B05/B05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>[{'sequence_level_annotation': True, 'id': '10...</td>\n",
       "      <td>resting</td>\n",
       "      <td>B05</td>\n",
       "      <td>2010-07-20 15:23:10</td>\n",
       "      <td>/data/luiz/dataset/serengeti_images/S1/B05/B05...</td>\n",
       "      <td>None</td>\n",
       "      <td>/data/luiz/dataset/serengeti_images/S1/B05/B05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>[{'sequence_level_annotation': True, 'id': '10...</td>\n",
       "      <td>eating</td>\n",
       "      <td>B05</td>\n",
       "      <td>2010-07-20 15:26:16</td>\n",
       "      <td>None</td>\n",
       "      <td>/data/luiz/dataset/serengeti_images/S1/B05/B05...</td>\n",
       "      <td>/data/luiz/dataset/serengeti_images/S1/B05/B05...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_frames                                             frames category  \\\n",
       "18           3  [{'sequence_level_annotation': True, 'id': '10...  resting   \n",
       "19           3  [{'sequence_level_annotation': True, 'id': '10...   moving   \n",
       "20           2  [{'sequence_level_annotation': True, 'id': '10...   eating   \n",
       "21           2  [{'sequence_level_annotation': True, 'id': '10...  resting   \n",
       "22           2  [{'sequence_level_annotation': True, 'id': '10...   eating   \n",
       "\n",
       "   location            datetime  \\\n",
       "18      B04 2010-08-12 16:28:50   \n",
       "19      B04 2010-08-20 03:37:00   \n",
       "20      B05 2010-07-20 15:19:52   \n",
       "21      B05 2010-07-20 15:23:10   \n",
       "22      B05 2010-07-20 15:26:16   \n",
       "\n",
       "                                           path_empty  \\\n",
       "18  /data/luiz/dataset/serengeti_images/S1/B04/B04...   \n",
       "19                                               None   \n",
       "20                                               None   \n",
       "21  /data/luiz/dataset/serengeti_images/S1/B05/B05...   \n",
       "22                                               None   \n",
       "\n",
       "                                          path_animal  \\\n",
       "18                                               None   \n",
       "19  /data/luiz/dataset/serengeti_images/S1/B04/B04...   \n",
       "20  /data/luiz/dataset/serengeti_images/S1/B05/B05...   \n",
       "21                                               None   \n",
       "22  /data/luiz/dataset/serengeti_images/S1/B05/B05...   \n",
       "\n",
       "                                             path_seq  \n",
       "18  /data/luiz/dataset/serengeti_images/S1/B04/B04...  \n",
       "19  /data/luiz/dataset/serengeti_images/S1/B04/B04...  \n",
       "20  /data/luiz/dataset/serengeti_images/S1/B05/B05...  \n",
       "21  /data/luiz/dataset/serengeti_images/S1/B05/B05...  \n",
       "22  /data/luiz/dataset/serengeti_images/S1/B05/B05...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original categories names: {7: 'gazellethomsons', 5: 'zebra', 16: 'hartebeest', 18: 'wildebeest', 15: 'buffalo', 2: 'gazellegrants', 11: 'elephant', 23: 'otherbird', 8: 'hyenaspotted', 21: 'lionfemale', 37: 'topi', 40: 'eland'}\n",
      "prompt: 0) gazellethomsons, 1) zebra, 2) hartebeest, 3) wildebeest, 4) buffalo, 5) gazellegrants, 6) elephant, 7) otherbird, 8) hyenaspotted, 9) lionfemale, 10) topi and 11) eland\n",
      "wildebeest         136627\n",
      "zebra               75555\n",
      "gazellethomsons     56266\n",
      "hartebeest          27941\n",
      "buffalo             17806\n",
      "otherbird           15140\n",
      "elephant            14247\n",
      "gazellegrants       13645\n",
      "hyenaspotted         7470\n",
      "lionfemale           5775\n",
      "eland                4894\n",
      "topi                 3750\n",
      "Name: category, dtype: int64\n",
      "/data/luiz/dataset/partitions/species-classifier/serengeti/train.csv 20000\n",
      "gazellegrants      1723\n",
      "topi               1697\n",
      "buffalo            1675\n",
      "wildebeest         1674\n",
      "zebra              1663\n",
      "eland              1662\n",
      "gazellethomsons    1661\n",
      "hyenaspotted       1660\n",
      "lionfemale         1657\n",
      "hartebeest         1654\n",
      "otherbird          1638\n",
      "elephant           1636\n",
      "Name: category, dtype: int64 \n",
      "\n",
      "/data/luiz/dataset/partitions/species-classifier/serengeti/val.csv 3000\n",
      "gazellegrants      266\n",
      "otherbird          260\n",
      "eland              257\n",
      "elephant           257\n",
      "hartebeest         252\n",
      "topi               249\n",
      "buffalo            249\n",
      "lionfemale         246\n",
      "hyenaspotted       244\n",
      "wildebeest         244\n",
      "gazellethomsons    241\n",
      "zebra              235\n",
      "Name: category, dtype: int64 \n",
      "\n",
      "/data/luiz/dataset/partitions/species-classifier/serengeti/test.csv 10000\n",
      "gazellegrants      870\n",
      "buffalo            853\n",
      "wildebeest         852\n",
      "topi               842\n",
      "elephant           841\n",
      "hyenaspotted       837\n",
      "hartebeest         832\n",
      "lionfemale         829\n",
      "eland              827\n",
      "zebra              810\n",
      "gazellethomsons    809\n",
      "otherbird          798\n",
      "Name: category, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def random_df(df, size):\n",
    "    return df.sample(n=size, random_state=SEED)\n",
    "\n",
    "\n",
    "def create_side_by_side_image(batchs):\n",
    "    response = []\n",
    "    for batch in tqdm(batchs):\n",
    "        output_path = f'{SEQUENCE_PATH}{uuid4()}{FILE_TYPE}'\n",
    "\n",
    "        images = [Image.open(img) for img in batch.split(\",\")]\n",
    "        min_height = min(img.height for img in images)\n",
    "        resized_images = [\n",
    "            img.resize((int(img.width * min_height / img.height), min_height), Image.Resampling.LANCZOS) for img in images\n",
    "        ]\n",
    "        total_width = sum(img.width for img in resized_images)\n",
    "        combined_image = Image.new(\"RGB\", (total_width, min_height), (255, 255, 255))\n",
    "        x_offset = 0\n",
    "        for img in resized_images:\n",
    "            combined_image.paste(img, (x_offset, 0))\n",
    "            x_offset += img.width\n",
    "        combined_image.save(output_path)\n",
    "        response.append(output_path)\n",
    "    return response\n",
    "\n",
    "def split_data_subsets(df, sub_dir):\n",
    "    unique_locations = df.location.value_counts().index.values\n",
    "    np.random.shuffle(unique_locations)\n",
    "    # Define partition ratios\n",
    "    train_ratio = 0.5\n",
    "    val_ratio = 0.15\n",
    "    # Calculate split indices\n",
    "    n_total = len(unique_locations)\n",
    "    train_end = int(train_ratio * n_total)\n",
    "    val_end = train_end + int(val_ratio * n_total)\n",
    "    # Split locations into partitions\n",
    "    train_locations = unique_locations[:train_end]\n",
    "    val_locations = unique_locations[train_end:val_end]\n",
    "    test_locations = unique_locations[val_end:]\n",
    "    # Assign partitions\n",
    "    train = df[df['location'].isin(train_locations)]\n",
    "    val = df[df['location'].isin(val_locations)]\n",
    "    test = df[df['location'].isin(test_locations)]\n",
    "    train = balance_dataset(train)\n",
    "    val = balance_dataset(val)\n",
    "    test = balance_dataset(test)\n",
    "    return random_df(train, 20000), random_df(val, 3000), random_df(test, 10000)\n",
    "\n",
    "def save_results(df, task, filename):\n",
    "    path = f\"{RESULTS_PATH}{task}/{DATABASE}/{filename}\"\n",
    "    print(path, len(df))\n",
    "    print(df.category.value_counts(), \"\\n\")\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "def save_animal_classifier_dataset(df):\n",
    "    df[\"category\"] = df.path_animal.map(lambda a: \"yes\" if isinstance(a, str) else \"no\")\n",
    "    df = df.rename(columns={\"path_animal\": \"path\"})\n",
    "    df[\"path\"] = df[\"path\"].combine_first(df[\"path_empty\"])\n",
    "    df = df[[\"num_frames\", \"category\", \"location\", \"datetime\", \"path\"]]\n",
    "    train, val, test = split_data_subsets(df)\n",
    "    save_results(train, \"animal-classifier\", \"train.csv\")\n",
    "    save_results(val, \"animal-classifier\", \"val.csv\")\n",
    "    save_results(test, \"animal-classifier\", \"test.csv\")\n",
    "\n",
    "\n",
    "def save_behaviour_classifier_dataset(df):\n",
    "    df = df.rename(columns={\"path_animal\": \"path\"})\n",
    "    df = df[[\"num_frames\", \"category\", \"location\", \"datetime\", \"path\", \"path_seq\"]].dropna()\n",
    "    train, val, test = split_data_subsets(df)\n",
    "\n",
    "    train[\"path_seq_saved\"] = create_side_by_side_image(train[\"path_seq\"])\n",
    "    val[\"path_seq_saved\"] = create_side_by_side_image(val[\"path_seq\"])\n",
    "    test[\"path_seq_saved\"] = create_side_by_side_image(test[\"path_seq\"])\n",
    "\n",
    "    save_results(train, \"behaviour-classifier\", \"train.csv\")\n",
    "    save_results(val, \"behaviour-classifier\", \"val.csv\")\n",
    "    save_results(test, \"behaviour-classifier\", \"test.csv\")\n",
    "\n",
    "def save_species_classifier_dataset(df):\n",
    "    def select_rows(categories):\n",
    "        return df[df['category'].isin(categories)]\n",
    "    \n",
    "    def filter_dataframe(df):\n",
    "        # Step 1: Calculate category occurrences\n",
    "        category_counts = df['category'].value_counts()\n",
    "        # Step 2: Divide categories into top, middle, and bottom groups\n",
    "        num_categories = 4\n",
    "        valid_categories = category_counts[category_counts >= 3000]\n",
    "        # Step 3: Divide valid categories into top, middle, and bottom groups\n",
    "        top_categories = valid_categories.head(num_categories).index\n",
    "        middle_categories = valid_categories.iloc[len(valid_categories)//2 - num_categories//2 : len(valid_categories)//2 + num_categories//2].index\n",
    "        bottom_categories = valid_categories.tail(num_categories).index\n",
    "        top_df = select_rows(top_categories)\n",
    "        middle_df = select_rows(middle_categories)\n",
    "        bottom_df = select_rows(bottom_categories)\n",
    "        # Step 4: Combine the results into a new DataFrame\n",
    "        return pd.concat([top_df, middle_df, bottom_df], ignore_index=True)\n",
    "    \n",
    "    def reset_categories_ids(df):\n",
    "        def build_prompt_by_dict(data):\n",
    "            items = [f\"{key}) {value}\" for key, value in data.items()]\n",
    "            if len(items) > 1:\n",
    "                return \", \".join(items[:-1]) + \" and \" + items[-1]\n",
    "            return items[0] if items else \"\"\n",
    "\n",
    "        def build_labels_mapper(categories):\n",
    "            mapper = {item[\"id\"]: item[\"name\"] for item in data[\"categories\"]}\n",
    "            return {category: mapper[category] for category in categories}\n",
    "\n",
    "        original = build_labels_mapper(df.category.unique())\n",
    "        print(\"original categories names:\", original)\n",
    "\n",
    "        replaced = {}\n",
    "        for idx, key in enumerate(original.keys()):\n",
    "            replaced[idx] = original[key]\n",
    "\n",
    "        df[\"category\"] = df[\"category\"].replace(original)\n",
    "        prompt = build_prompt_by_dict(replaced)\n",
    "        \n",
    "        print(\"prompt:\", prompt)\n",
    "        return df\n",
    "\n",
    "    concatenated = [item for sublist in df.frames.values for item in sublist]\n",
    "    df = pd.DataFrame(concatenated)\n",
    "    df = df.rename(columns={\"category_id\": \"category\"})\n",
    "    df = df[[\"category\", \"location\", \"datetime\", \"path\"]]\n",
    "    df = df[~df['category'].isin([0, 1])]\n",
    "    df = filter_dataframe(df)\n",
    "    df = reset_categories_ids(df)\n",
    "\n",
    "    print(df.category.value_counts())\n",
    "\n",
    "    train, val, test = split_data_subsets(df)\n",
    "\n",
    "    save_results(train, \"species-classifier\", \"train.csv\")\n",
    "    save_results(val, \"species-classifier\", \"val.csv\")\n",
    "    save_results(test, \"species-classifier\", \"test.csv\")\n",
    "\n",
    "save_species_classifier_dataset(df.copy())\n",
    "# save_animal_classifier_dataset(df.copy())\n",
    "# save_behaviour_classifier_dataset(df.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PASSING IMAGES TO SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "RESULTS_PATH = \"/data/luiz/dataset/partitions/\"\n",
    "\n",
    "SEED = 10\n",
    "\n",
    "def list_all_csv_files(directory):\n",
    "    csv_files = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                csv_files.append(os.path.join(root, file))\n",
    "    return csv_files\n",
    "\n",
    "csv_files = list_all_csv_files(RESULTS_PATH)\n",
    "ssd_images = []\n",
    "for file in csv_files:\n",
    "    ssd_images.extend(pd.read_csv(file)[\"path\"])\n",
    "\n",
    "# ssd_images = list(set(ssd_images))\n",
    "len(ssd_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REPLACE DIR HD TO DIR SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df[\"path\"] = df[\"path\"].map(lambda a: a.replace(\"/data/\", \"/ssd/\"))\n",
    "    df.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COPY IMAGES TO SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32320/32320 [12:04<00:00, 44.62it/s] \n"
     ]
    }
   ],
   "source": [
    "def copy_images_to_ssd(images):\n",
    "    for file_name in tqdm(list(set(images))):\n",
    "        file_name_ssd = file_name.replace(\"/data/\", \"/ssd/\")\n",
    "        file_name_hd = file_name.replace(\"/ssd/\", \"/data/\")\n",
    "        destination_dir = os.path.dirname(file_name_ssd)\n",
    "        if not os.path.exists(destination_dir):\n",
    "            os.makedirs(destination_dir)\n",
    "        shutil.copy(file_name_hd, file_name_ssd)\n",
    "\n",
    "copy_images_to_ssd(ssd_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "luiz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
