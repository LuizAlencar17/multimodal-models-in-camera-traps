{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from uuid import uuid4\n",
    "\n",
    "FILE_TYPE = \".JPG\"\n",
    "IMAGES_PATH = \"/data/luiz/dataset/serengeti_images/\"\n",
    "ANNOTATIONS_PATH = \"/data/luiz/dataset/serengeti/SnapshotSerengeti_S1-11_v2.1.json\"\n",
    "SEQUENCE_PATH = \"/ssd/luiz/dataset/sequences/\"\n",
    "RESULTS_PATH = \"/data/luiz/dataset/partitions/\"\n",
    "ANNOTATIONS_PATH_CSV = \"/data/luiz/dataset/serengeti/SnapshotSerengeti_v2_1_annotations.csv\"\n",
    "DATABASE = \"serengeti\"\n",
    "CSV_FIELDS = {\n",
    "    'capture_id': 'id',\n",
    "    'question__standing': 'standing', \n",
    "    'question__resting': 'resting', \n",
    "    'question__moving': 'moving', \n",
    "    'question__eating': 'eating', \n",
    "    'question__interacting': 'interacting'\n",
    "}\n",
    "USE_CSV = True\n",
    "\n",
    "SEED = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30830/234524135.py:4: DtypeWarning: Columns (8,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_csv = pd.read_csv(ANNOTATIONS_PATH_CSV)[CSV_FIELDS.keys()]\n"
     ]
    }
   ],
   "source": [
    "data = json.load(open(ANNOTATIONS_PATH))\n",
    "\n",
    "if USE_CSV:\n",
    "    df_csv = pd.read_csv(ANNOTATIONS_PATH_CSV)[CSV_FIELDS.keys()]\n",
    "    df_data = pd.DataFrame(data[\"annotations\"])\n",
    "    df_data = pd.merge(df_data, df_csv, left_on='seq_id', right_on='capture_id', how='inner')\n",
    "\n",
    "    df_data = df_data.rename(columns=CSV_FIELDS)\n",
    "    # Remover duplicatas mantendo a primeira ocorrência\n",
    "    df_data = df_data.loc[:, ~df_data.columns.duplicated()]\n",
    "\n",
    "data[\"annotations\"] = df_data.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting sequence mapper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 14500/7429835 [00:00<00:51, 144986.31it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7429835/7429835 [01:17<00:00, 96034.34it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting categories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1033219/1033219 [00:11<00:00, 92183.48it/s]\n"
     ]
    }
   ],
   "source": [
    "ALL_ACTIONS = ['standing', 'resting', 'moving', 'eating', 'interacting']\n",
    "ACTIONS = ['resting', 'moving', 'eating']\n",
    "CATEGORIES_INVALID = [1]\n",
    "MAX_ANIMALS = 1\n",
    "\n",
    "def get_category_from_sequence(sequence):\n",
    "    mapper = {}\n",
    "    for action in ALL_ACTIONS:\n",
    "        for frame in sequence:\n",
    "            mapper[action] = mapper.get(action, 0) + frame.get(action, 0)\n",
    "    return max(mapper, key=mapper.get)\n",
    "\n",
    "def is_valid_frame(frame):\n",
    "    try:\n",
    "        count = int(frame[\"count\"])\n",
    "    except Exception:\n",
    "        count = 0\n",
    "    exist_file = os.path.isfile(frame[\"path\"])\n",
    "    category_invalid = frame[\"category_id\"] in CATEGORIES_INVALID\n",
    "    count_valid = count <= MAX_ANIMALS\n",
    "    if not exist_file or category_invalid or not count_valid:\n",
    "        return False\n",
    "    # for action in ALL_ACTIONS:\n",
    "    #     try:\n",
    "    #         if not frame.get(action) >= 0:\n",
    "    #             return False\n",
    "    #     except Exception:\n",
    "    #         return False\n",
    "    return True\n",
    "\n",
    "def get_sequence_mapper(annotations):\n",
    "    mapper = {}\n",
    "    print(\"getting sequence mapper\")\n",
    "    for item in tqdm(annotations):\n",
    "        id = item.get(\"seq_id\")\n",
    "        item[\"path\"] = f'{IMAGES_PATH}{item[\"image_id\"]}{FILE_TYPE}'\n",
    "        if is_valid_frame(item):\n",
    "            if not mapper.get(id):\n",
    "                mapper[id] = []\n",
    "            mapper[id].append(item)\n",
    "    return mapper\n",
    "\n",
    "def get_frames_sequences(data):\n",
    "    sequence_mapper = get_sequence_mapper(data[\"annotations\"])\n",
    "    events = []\n",
    "    print(\"getting categories\")\n",
    "    for key in tqdm(sequence_mapper.keys()):\n",
    "        frames = sequence_mapper[key]\n",
    "        events.append({\n",
    "            \"num_frames\": len(frames),\n",
    "            \"frames\": frames,\n",
    "            \"datetime\": frames[0][\"datetime\"],\n",
    "            \"category\": get_category_from_sequence(frames)})\n",
    "    return events\n",
    "\n",
    "sequences = get_frames_sequences(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['resting', 'moving', 'eating'], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(sequences)\n",
    "df = df[df.num_frames > 1]\n",
    "df = df.replace(\"interacting\", \"moving\").replace(\"standing\", \"resting\")\n",
    "df.category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(df):\n",
    "    dfs = []\n",
    "    size = min(df.category.value_counts())\n",
    "    for category in df.category.unique():\n",
    "        filtered = df[df.category == category].sample(size, random_state=SEED)\n",
    "        dfs.append(filtered)\n",
    "    new_df = pd.concat(dfs).reset_index(drop=True)\n",
    "    return new_df.sample(len(new_df), random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "def get_empty_in_frames(frames):\n",
    "    for frame in frames:\n",
    "        if frame[\"category_id\"] == 0:\n",
    "            return frame[\"path\"]\n",
    "    return None\n",
    "\n",
    "def get_animal_in_frames(frames):\n",
    "    for frame in frames:\n",
    "        if frame[\"category_id\"] != 0:\n",
    "            return frame[\"path\"]\n",
    "    return None\n",
    "\n",
    "df[\"path_empty\"] = df.frames.map(lambda a: get_empty_in_frames(a))\n",
    "df[\"path_animal\"] = df.frames.map(lambda a: get_animal_in_frames(a))\n",
    "df['location'] = df.frames.map(lambda a: a[0]['location'])\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df['path_seq'] = df.frames.map(lambda a: \",\".join([item[\"path\"] for item in a]))\n",
    "\n",
    "df = df[[\"num_frames\", \"frames\", \"category\", \"location\", \"datetime\", \"path_empty\", \"path_animal\", \"path_seq\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "resting    792905\n",
       "moving      56684\n",
       "eating      28968\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = balance_dataset(df)\n",
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_frames</th>\n",
       "      <th>frames</th>\n",
       "      <th>category</th>\n",
       "      <th>location</th>\n",
       "      <th>datetime</th>\n",
       "      <th>path_empty</th>\n",
       "      <th>path_animal</th>\n",
       "      <th>path_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>[{'sequence_level_annotation': True, 'id': '10...</td>\n",
       "      <td>resting</td>\n",
       "      <td>B04</td>\n",
       "      <td>2010-08-12 16:28:50</td>\n",
       "      <td>/data/luiz/dataset/serengeti_images/S1/B04/B04...</td>\n",
       "      <td>None</td>\n",
       "      <td>/data/luiz/dataset/serengeti_images/S1/B04/B04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>[{'sequence_level_annotation': True, 'id': '10...</td>\n",
       "      <td>moving</td>\n",
       "      <td>B04</td>\n",
       "      <td>2010-08-20 03:37:00</td>\n",
       "      <td>None</td>\n",
       "      <td>/data/luiz/dataset/serengeti_images/S1/B04/B04...</td>\n",
       "      <td>/data/luiz/dataset/serengeti_images/S1/B04/B04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>[{'sequence_level_annotation': True, 'id': '10...</td>\n",
       "      <td>eating</td>\n",
       "      <td>B05</td>\n",
       "      <td>2010-07-20 15:19:52</td>\n",
       "      <td>None</td>\n",
       "      <td>/data/luiz/dataset/serengeti_images/S1/B05/B05...</td>\n",
       "      <td>/data/luiz/dataset/serengeti_images/S1/B05/B05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>[{'sequence_level_annotation': True, 'id': '10...</td>\n",
       "      <td>resting</td>\n",
       "      <td>B05</td>\n",
       "      <td>2010-07-20 15:23:10</td>\n",
       "      <td>/data/luiz/dataset/serengeti_images/S1/B05/B05...</td>\n",
       "      <td>None</td>\n",
       "      <td>/data/luiz/dataset/serengeti_images/S1/B05/B05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>[{'sequence_level_annotation': True, 'id': '10...</td>\n",
       "      <td>eating</td>\n",
       "      <td>B05</td>\n",
       "      <td>2010-07-20 15:26:16</td>\n",
       "      <td>None</td>\n",
       "      <td>/data/luiz/dataset/serengeti_images/S1/B05/B05...</td>\n",
       "      <td>/data/luiz/dataset/serengeti_images/S1/B05/B05...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_frames                                             frames category  \\\n",
       "18           3  [{'sequence_level_annotation': True, 'id': '10...  resting   \n",
       "19           3  [{'sequence_level_annotation': True, 'id': '10...   moving   \n",
       "20           2  [{'sequence_level_annotation': True, 'id': '10...   eating   \n",
       "21           2  [{'sequence_level_annotation': True, 'id': '10...  resting   \n",
       "22           2  [{'sequence_level_annotation': True, 'id': '10...   eating   \n",
       "\n",
       "   location            datetime  \\\n",
       "18      B04 2010-08-12 16:28:50   \n",
       "19      B04 2010-08-20 03:37:00   \n",
       "20      B05 2010-07-20 15:19:52   \n",
       "21      B05 2010-07-20 15:23:10   \n",
       "22      B05 2010-07-20 15:26:16   \n",
       "\n",
       "                                           path_empty  \\\n",
       "18  /data/luiz/dataset/serengeti_images/S1/B04/B04...   \n",
       "19                                               None   \n",
       "20                                               None   \n",
       "21  /data/luiz/dataset/serengeti_images/S1/B05/B05...   \n",
       "22                                               None   \n",
       "\n",
       "                                          path_animal  \\\n",
       "18                                               None   \n",
       "19  /data/luiz/dataset/serengeti_images/S1/B04/B04...   \n",
       "20  /data/luiz/dataset/serengeti_images/S1/B05/B05...   \n",
       "21                                               None   \n",
       "22  /data/luiz/dataset/serengeti_images/S1/B05/B05...   \n",
       "\n",
       "                                             path_seq  \n",
       "18  /data/luiz/dataset/serengeti_images/S1/B04/B04...  \n",
       "19  /data/luiz/dataset/serengeti_images/S1/B04/B04...  \n",
       "20  /data/luiz/dataset/serengeti_images/S1/B05/B05...  \n",
       "21  /data/luiz/dataset/serengeti_images/S1/B05/B05...  \n",
       "22  /data/luiz/dataset/serengeti_images/S1/B05/B05...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original categories names: {8: 'hyenaspotted', 5: 'zebra', 13: 'giraffe', 15: 'buffalo', 2: 'gazellegrants', 18: 'wildebeest', 11: 'elephant', 21: 'lionfemale', 23: 'otherbird'}\n",
      "{0: 'hyena', 1: 'zebra', 2: 'giraffe', 3: 'buffalo', 4: 'gazelle', 5: 'wildebeest', 6: 'elephant', 7: 'lion', 8: 'bird'}\n",
      "prompt: 0) hyena, 1) zebra, 2) giraffe, 3) buffalo, 4) gazelle, 5) wildebeest, 6) elephant, 7) lion and 8) bird\n",
      "wildebeest    136627\n",
      "zebra          75555\n",
      "giraffe        18477\n",
      "buffalo        17806\n",
      "bird           15140\n",
      "elephant       14247\n",
      "gazelle        13645\n",
      "hyena           7470\n",
      "lion            5775\n",
      "Name: category, dtype: int64\n",
      "/data/luiz/dataset/partitions/species-classifier/serengeti/train.csv 20000\n",
      "elephant      2257\n",
      "buffalo       2248\n",
      "zebra         2245\n",
      "gazelle       2238\n",
      "lion          2220\n",
      "hyena         2219\n",
      "wildebeest    2209\n",
      "giraffe       2203\n",
      "bird          2161\n",
      "Name: category, dtype: int64 \n",
      "\n",
      "/data/luiz/dataset/partitions/species-classifier/serengeti/val.csv 3000\n",
      "gazelle       350\n",
      "hyena         347\n",
      "elephant      341\n",
      "lion          334\n",
      "bird          333\n",
      "buffalo       329\n",
      "zebra         323\n",
      "wildebeest    322\n",
      "giraffe       321\n",
      "Name: category, dtype: int64 \n",
      "\n",
      "/data/luiz/dataset/partitions/species-classifier/serengeti/test.csv 10000\n",
      "buffalo       1136\n",
      "hyena         1130\n",
      "bird          1127\n",
      "lion          1125\n",
      "wildebeest    1122\n",
      "zebra         1111\n",
      "giraffe       1103\n",
      "gazelle       1075\n",
      "elephant      1071\n",
      "Name: category, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def random_df(df, size):\n",
    "    return df.sample(n=size, random_state=SEED)\n",
    "\n",
    "\n",
    "def create_side_by_side_image(batchs):\n",
    "    response = []\n",
    "    for batch in tqdm(batchs):\n",
    "        output_path = f'{SEQUENCE_PATH}{uuid4()}{FILE_TYPE}'\n",
    "\n",
    "        images = [Image.open(img) for img in batch.split(\",\")]\n",
    "        min_height = min(img.height for img in images)\n",
    "        resized_images = [\n",
    "            img.resize((int(img.width * min_height / img.height), min_height), Image.Resampling.LANCZOS) for img in images\n",
    "        ]\n",
    "        total_width = sum(img.width for img in resized_images)\n",
    "        combined_image = Image.new(\"RGB\", (total_width, min_height), (255, 255, 255))\n",
    "        x_offset = 0\n",
    "        for img in resized_images:\n",
    "            combined_image.paste(img, (x_offset, 0))\n",
    "            x_offset += img.width\n",
    "        combined_image.save(output_path)\n",
    "        response.append(output_path)\n",
    "    return response\n",
    "\n",
    "def split_data_subsets(df):\n",
    "    unique_locations = df.location.value_counts().index.values\n",
    "    np.random.shuffle(unique_locations)\n",
    "    # Define partition ratios\n",
    "    train_ratio = 0.5\n",
    "    val_ratio = 0.15\n",
    "    # Calculate split indices\n",
    "    n_total = len(unique_locations)\n",
    "    train_end = int(train_ratio * n_total)\n",
    "    val_end = train_end + int(val_ratio * n_total)\n",
    "    # Split locations into partitions\n",
    "    train_locations = unique_locations[:train_end]\n",
    "    val_locations = unique_locations[train_end:val_end]\n",
    "    test_locations = unique_locations[val_end:]\n",
    "    # Assign partitions\n",
    "    train = df[df['location'].isin(train_locations)]\n",
    "    val = df[df['location'].isin(val_locations)]\n",
    "    test = df[df['location'].isin(test_locations)]\n",
    "    train = balance_dataset(train)\n",
    "    val = balance_dataset(val)\n",
    "    test = balance_dataset(test)\n",
    "    return random_df(train, 20000), random_df(val, 3000), random_df(test, 10000)\n",
    "\n",
    "def save_results(df, task, filename):\n",
    "    path = f\"{RESULTS_PATH}{task}/{DATABASE}/{filename}\"\n",
    "    print(path, len(df))\n",
    "    print(df.category.value_counts(), \"\\n\")\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "def save_animal_classifier_dataset(df):\n",
    "    df[\"category\"] = df.path_animal.map(lambda a: \"yes\" if isinstance(a, str) else \"no\")\n",
    "    df = df.rename(columns={\"path_animal\": \"path\"})\n",
    "    df[\"path\"] = df[\"path\"].combine_first(df[\"path_empty\"])\n",
    "    df = df[[\"num_frames\", \"category\", \"location\", \"datetime\", \"path\"]]\n",
    "    train, val, test = split_data_subsets(df)\n",
    "    save_results(train, \"animal-classifier\", \"train.csv\")\n",
    "    save_results(val, \"animal-classifier\", \"val.csv\")\n",
    "    save_results(test, \"animal-classifier\", \"test.csv\")\n",
    "\n",
    "\n",
    "def save_behaviour_classifier_dataset(df):\n",
    "    df = df.rename(columns={\"path_animal\": \"path\"})\n",
    "    df = df[[\"num_frames\", \"category\", \"location\", \"datetime\", \"path\", \"path_seq\"]].dropna()\n",
    "    train, val, test = split_data_subsets(df)\n",
    "\n",
    "    train[\"path_seq_saved\"] = create_side_by_side_image(train[\"path_seq\"])\n",
    "    val[\"path_seq_saved\"] = create_side_by_side_image(val[\"path_seq\"])\n",
    "    test[\"path_seq_saved\"] = create_side_by_side_image(test[\"path_seq\"])\n",
    "\n",
    "    save_results(train, \"behaviour-classifier\", \"train.csv\")\n",
    "    save_results(val, \"behaviour-classifier\", \"val.csv\")\n",
    "    save_results(test, \"behaviour-classifier\", \"test.csv\")\n",
    "\n",
    "def save_species_classifier_dataset(df):\n",
    "    def build_prompt_by_dict(data):\n",
    "        new_mapper = {idx: category for idx, category in enumerate(data)}\n",
    "        print(new_mapper)\n",
    "        items = [f\"{idx}) {value}\" for idx, value in enumerate(data)]\n",
    "        if len(items) > 1:\n",
    "            return \", \".join(items[:-1]) + \" and \" + items[-1]\n",
    "        return items[0] if items else \"\"\n",
    "\n",
    "    def build_labels_mapper(categories):\n",
    "        mapper = {item[\"id\"]: item[\"name\"] for item in data[\"categories\"]}\n",
    "        return {category: mapper[category] for idx, category in enumerate(categories)}\n",
    "\n",
    "    concatenated = [item for sublist in df.frames.values for item in sublist]\n",
    "    df = pd.DataFrame(concatenated)\n",
    "    df = df.rename(columns={\"category_id\": \"category\"})\n",
    "    df = df[[\"category\", \"location\", \"datetime\", \"path\"]]\n",
    "\n",
    "    species = {\n",
    "        'gazellegrants',\n",
    "        'elephant',\n",
    "        'lionfemale',\n",
    "        'giraffe',\n",
    "        'zebra',\n",
    "        'buffalo',\n",
    "        'wildebeest',\n",
    "        'hyenaspotted',\n",
    "        'wilddog',\n",
    "        'otherbird'\n",
    "    }\n",
    "\n",
    "    categories_id = {item[\"id\"] for item in data[\"categories\"] if item[\"name\"] in species}\n",
    "    df = df[df['category'].isin(categories_id)]\n",
    "\n",
    "    original = build_labels_mapper(df.category.unique())\n",
    "    print(\"original categories names:\", original)\n",
    "\n",
    "    df[\"category\"] = df[\"category\"].replace(original)\n",
    "    df[\"category\"] = df[\"category\"].replace({\n",
    "        'lionfemale': 'lion',\n",
    "        'otherbird': 'bird',\n",
    "        'gazellegrants': 'gazelle',\n",
    "        'hyenaspotted': 'hyena'\n",
    "    })\n",
    "    prompt = build_prompt_by_dict(df[\"category\"].unique())\n",
    "    print(\"prompt:\", prompt)\n",
    "    print(df.category.value_counts())\n",
    "\n",
    "    train, val, test = split_data_subsets(df)\n",
    "\n",
    "    save_results(train, \"species-classifier\", \"train.csv\")\n",
    "    save_results(val, \"species-classifier\", \"val.csv\")\n",
    "    save_results(test, \"species-classifier\", \"test.csv\")\n",
    "\n",
    "save_species_classifier_dataset(df.copy())\n",
    "# save_animal_classifier_dataset(df.copy())\n",
    "# save_behaviour_classifier_dataset(df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PASSING IMAGES TO SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "RESULTS_PATH = \"/data/luiz/dataset/partitions/species-classifier/\"\n",
    "\n",
    "SEED = 10\n",
    "\n",
    "def list_all_csv_files(directory):\n",
    "    csv_files = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                csv_files.append(os.path.join(root, file))\n",
    "    return csv_files\n",
    "\n",
    "csv_files = list_all_csv_files(RESULTS_PATH)\n",
    "ssd_images = []\n",
    "for file in csv_files:\n",
    "    ssd_images.extend(pd.read_csv(file)[\"path\"])\n",
    "\n",
    "# ssd_images = list(set(ssd_images))\n",
    "len(ssd_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REPLACE DIR HD TO DIR SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df[\"path\"] = df[\"path\"].map(lambda a: a.replace(\"/data/\", \"/ssd/\"))\n",
    "    df.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COPY IMAGES TO SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32380/32380 [11:50<00:00, 45.54it/s] \n"
     ]
    }
   ],
   "source": [
    "def copy_images_to_ssd(images):\n",
    "    for file_name in tqdm(list(set(images))):\n",
    "        file_name_ssd = file_name.replace(\"/data/\", \"/ssd/\")\n",
    "        file_name_hd = file_name.replace(\"/ssd/\", \"/data/\")\n",
    "        destination_dir = os.path.dirname(file_name_ssd)\n",
    "        if not os.path.exists(destination_dir):\n",
    "            os.makedirs(destination_dir)\n",
    "        shutil.copy(file_name_hd, file_name_ssd)\n",
    "\n",
    "copy_images_to_ssd(ssd_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "luiz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
